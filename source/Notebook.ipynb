{
 "cells": [
  {
   "source": [
    "## Construção do dataframe utilizando buscas no banco de dados sql\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "from data_base.models import models\n",
    "from data_base.connection import session\n",
    "\n",
    "query = select(\n",
    "    models.Variables.date,\n",
    "    models.Variables.precipitation.label('precipitation'),\n",
    "    models.Variables.temperature.label('temperature'),\n",
    "    models.Variables.evaporation.label('evaporation'),\n",
    "    models.Variables.surface_runoff.label('surface_runoff'),\n",
    "    models.Coordinate.river_id.label('river'),    \n",
    "    models.Reservoir.level,\n",
    "    models.Reservoir.streamflow\n",
    ").\\\n",
    "    join(models.Variables.coordinate).\\\n",
    "    join(models.Reservoir, models.Variables.date == models.Reservoir.date)\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "RawDataFrame = pandas.read_sql(query, session.bind)\n"
   ]
  },
  {
   "source": [
    "# DataFrame consolidado porém com os atributos para cada rio posicionados em uma diferente coluna\n",
    "ConsolidatedDataFrame = (\n",
    "    RawDataFrame.\n",
    "    groupby(['date', 'river', 'level', 'streamflow']).\n",
    "    agg({\n",
    "        'precipitation': 'sum',\n",
    "        'evaporation': 'sum',\n",
    "        'temperature': 'mean',\n",
    "        'surface_runoff':'mean',\n",
    "    }).\n",
    "    reset_index().\n",
    "    pivot(index=[\"date\", 'level', 'streamflow'], columns=\"river\")\n",
    ")\n",
    "\n",
    "ConsolidatedDataFrame.insert(0,'previous_streamflow', pandas.DataFrame(ConsolidatedDataFrame.index.get_level_values('streamflow')).shift(1).values)\n",
    "ConsolidatedDataFrame.insert(0,'previous_level', pandas.DataFrame(ConsolidatedDataFrame.index.get_level_values('level')).shift(1).values)\n",
    "\n",
    "ConsolidatedDataFrame = ConsolidatedDataFrame.dropna()\n",
    "ConsolidatedDataFrame\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ConsolidatedDataFrame - ConsolidatedDataFrame.min()) / (ConsolidatedDataFrame.max() - ConsolidatedDataFrame.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from custom_transfomers.date_window import TimeWindowTransformer"
   ]
  },
  {
   "source": [
    "seed = 0\n",
    "scorer = make_scorer(accuracy_score) # Teste outras\n",
    "\n",
    "cross_validation_10 = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "cross_validation_3 = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_utils.data_manipulation import generate_aggregation\n",
    "rivers = session.query(models.River).all()\n",
    "\n",
    "precipitation_agg = generate_aggregation('sum', 'precipitation', [river.id for river in rivers])\n",
    "evaporation_agg = generate_aggregation('sum', 'evaporation', [river.id for river in rivers])\n",
    "temperature_agg = generate_aggregation('mean', 'temperature', [river.id for river in rivers])\n",
    "runoff_agg = generate_aggregation('sum', 'surface_runoff', [river.id for river in rivers])\n",
    "\n",
    "cols = ['precipitation', 'evaporation', 'temperature', 'surface_runoff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = precipitation_agg\n",
    "agg.update(evaporation_agg)\n",
    "agg.update(temperature_agg)\n",
    "agg.update(runoff_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "algorithms = {\n",
    "    # testar sem hiperparâmetros\n",
    "    'SVR':  GridSearchCV(\n",
    "            Pipeline([\n",
    "                ('windowing', TimeWindowTransformer(columns=cols)),\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "                ('transformer', TransformedTargetRegressor(\n",
    "                        transformer=MinMaxScaler(feature_range=(0, 1)), \n",
    "                        regressor=SVR(kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=- 1)\n",
    "                    )\n",
    "                )]), \n",
    "            param_grid={\n",
    "                'windowing__aggregate': [agg],\n",
    "                'windowing__rolling': range(1, 30, 5),\n",
    "                'windowing__dropna': [False],\n",
    "                'transformer__regressor__C': range(1, 15, 3)\n",
    "            },\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            n_jobs=multiprocessing.cpu_count()-1,\n",
    "            verbose=10,\n",
    "            error_score='raise'\n",
    "        ),\n",
    "    'RandomForest':  GridSearchCV(\n",
    "            Pipeline([\n",
    "                ('windowing', TimeWindowTransformer(columns=cols)),\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('random_forest', RandomForestRegressor(random_state=seed))]), \n",
    "            param_grid={\n",
    "                'windowing__aggregate': [agg],\n",
    "                'windowing__rolling': range(1, 30, 5),\n",
    "                'windowing__dropna': [False],\n",
    "                'random_forest__max_depth': range(1, 20, 5)\n",
    "            },\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            n_jobs=multiprocessing.cpu_count()-1,\n",
    "            verbose=10,\n",
    "            error_score='raise'\n",
    "        ),\n",
    "    'NormalizedRandomForest':  GridSearchCV(\n",
    "            Pipeline([\n",
    "                ('windowing', TimeWindowTransformer(columns=cols)),\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', MinMaxScaler(feature_range=(0, 1))),\n",
    "                ('transformer', TransformedTargetRegressor(\n",
    "                        transformer=MinMaxScaler(feature_range=(0, 1)),\n",
    "                        regressor=RandomForestRegressor(random_state=seed)\n",
    "                    )\n",
    "                )\n",
    "            ]), \n",
    "            param_grid={\n",
    "                'windowing__aggregate': [agg],\n",
    "                'windowing__rolling': range(1, 30, 5),\n",
    "                'windowing__dropna': [False],\n",
    "                'transformer__regressor__max_depth': range(1, 20, 5)\n",
    "            },\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=10,\n",
    "            error_score='raise'\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "result_level = {}\n",
    "for alg, clf in algorithms.items():\n",
    "    result_level[alg] = cross_val_score(clf, ConsolidatedDataFrame, ConsolidatedDataFrame.index.get_level_values('level'))\n",
    "    result_level['target'] = 'level'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_streamflow = {}\n",
    "for alg, clf in algorithms.items():\n",
    "    result_streamflow[alg] = cross_val_score(clf, ConsolidatedDataFrame, ConsolidatedDataFrame.index.get_level_values('streamflow'))\n",
    "    result_streamflow['target'] = 'streamflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_streamflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.DataFrame.from_dict(result_streamflow).append(pandas.DataFrame.from_dict(result_level))\n",
    "result.pivot(columns='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5  ('.env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "e01c415138263b81e74f4b8560efc462ff3e43faa8a63c42675b82c8ccb72528"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}